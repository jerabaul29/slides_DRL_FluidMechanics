\BOOKMARK [2][]{Outline0.1}{A short reminder about Artificial Neural Networks and Supervised Learning}{}% 1
\BOOKMARK [2][]{Outline0.2}{From Supervised to Reinforcement Learning}{}% 2
\BOOKMARK [2][]{Outline0.3}{i. Q-learning}{}% 3
\BOOKMARK [2][]{Outline0.4}{ii. Policy Gradient method}{}% 4
\BOOKMARK [2][]{Outline0.5}{iii. Refinements and amelioration of RL algorithms}{}% 5
\BOOKMARK [2][]{Outline0.6}{iv. Frameworks for DRL}{}% 6
\BOOKMARK [2][]{Outline0.7}{v. Why relevant for future}{}% 7
\BOOKMARK [2][]{Outline0.8}{Deep Reinforcement Learning for Active Flow Control}{}% 8
\BOOKMARK [2][]{Outline0.9}{DRL for shape optimization}{}% 9
\BOOKMARK [2][]{Outline0.10}{Other applications in the literature}{}% 10
\BOOKMARK [2][]{Outline0.11}{i. Rayleigh-Benard instability control}{}% 11
\BOOKMARK [2][]{Outline0.12}{ii. Optimal navigation: the Zemelo problem}{}% 12
\BOOKMARK [2][]{Outline0.13}{iii. Optimal swimming of fishes}{}% 13
\BOOKMARK [2][]{Outline0.14}{iv. Control of chaotic systems}{}% 14
\BOOKMARK [2][]{Outline0.15}{v. Control of AUVs}{}% 15
\BOOKMARK [2][]{Outline0.16}{vi. Control of glider}{}% 16
\BOOKMARK [2][]{Outline0.17}{vii. Optimal design of missiles}{}% 17
\BOOKMARK [2][]{Outline0.18}{viii. Growing interest}{}% 18
\BOOKMARK [2][]{Outline0.19}{Conclusion}{}% 19
